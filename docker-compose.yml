services:
  ollama:
    # Use your custom Dockerfile to build the Ollama service
    build: 
      context: ./ollama_server
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - .:/app
      - ./ollama_data:/root/.ollama  # âœ… Changed from direct /root path to local folder
      - ./ollama_models:/root/.ollama/models
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      # Keep the health check, but give it more time to start up and pull the model
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 15s
      retries: 5
      start_period: 120s
    deploy: # for laptop nvidia GPU support if not available comment out
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  mcp_server:
    build:
      context: ./mcp_server
      dockerfile: Dockerfile.mcp
    container_name: mcp_server
    ports:
      - "4000:4000"
    depends_on:
      # MCP server needs Ollama to be initialized to use the models
      ollama:
        condition: service_healthy
    restart: unless-stopped

  adk_app:
    build:
      context: ./adk_ollama_tool
      dockerfile: Dockerfile.adk
    container_name: adk_app
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MCP_SERVER_URL=http://mcp_server:4000
    depends_on:
      # ADK app needs Ollama to be initialized and MCP server to be running
      ollama:
        condition: service_healthy
      mcp_server:
        condition: service_started # ADK app just needs MCP to be running
    restart: unless-stopped

volumes:
  ollama_data:
    driver: local
  ollama_models:
    driver: local
  